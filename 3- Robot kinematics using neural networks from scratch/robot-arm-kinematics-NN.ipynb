{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import cupy as np\n",
    "# cupy is similar to numpy with gpu acceleration, if you have gpu acceleration, CUDNN, and supported engine, import cupy as np and remove import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def relu_backward(dA,Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ \n",
    "\n",
    "def same(Z):\n",
    "    return Z\n",
    "\n",
    "def same_backward(dA,Z):\n",
    "    return  dA # 1 x dA\n",
    "\n",
    "def tanh(Z):\n",
    "    return (np.exp(Z) - np.exp(-Z))/ (np.exp(Z) + np.exp(-Z))\n",
    "\n",
    "def tanh_backward(dA,Z):\n",
    "    \n",
    "    return dA*(1 - np.power(tanh(Z),2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and error Functions.\n",
    "\n",
    "def return_loss(yhat,y):\n",
    "    # MAE\n",
    "    loss=np.sum(np.abs(yhat - y))/yhat.shape[1]\n",
    "\n",
    "    return loss\n",
    "\n",
    "def error(yhat,y):\n",
    "\n",
    "    nm= np.subtract(yhat,y) \n",
    "\n",
    "    return nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a list for  Initialize entry architecture using a dictionary\n",
    "\n",
    "def dict_layers(encoded_list):\n",
    "    \"\"\"\n",
    "    Takes a list of dict, each list contains input_size,output_size,activation for a single layer.\n",
    "    Returns list of dict,\n",
    "    This will help in tracking multiple architectures while easing the intializing of the network\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    network_architechture_list = []\n",
    "\n",
    "    for item in encoded_list:\n",
    "        network_architechture_list.append({\"input_size\" : item[0] ,\"output_size\" : item[1], \"activation_func\" : item[2]})\n",
    "\n",
    "\n",
    "    return network_architechture_list\n",
    "\n",
    "# Takes the neural netork architecture and intializes weights and biases\n",
    "\n",
    "def initialize_nn_layers(nn_architecture,weight_scaling=0.15):\n",
    "    \n",
    "\n",
    "    params_values = {}\n",
    "    np.random.seed(2)\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        idx = idx + 1\n",
    "        input_size = layer[\"input_size\"]\n",
    "        output_size = layer[\"output_size\"]\n",
    "        \n",
    "        params_values['Weights' + str(idx)] = np.random.randn(output_size, input_size) * weight_scaling\n",
    "        params_values['bias' + str(idx)] = np.random.randn(output_size, 1) \n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs forward propagation on one layer only\n",
    "\n",
    "def one_layer_forward(input, weight, bias, activation_func):\n",
    "\n",
    "    Z = np.dot(weight, input) + bias\n",
    "    \n",
    "    if activation_func == \"same\":\n",
    "        func = same\n",
    "    elif activation_func == \"sigmoid\":\n",
    "        func = sigmoid\n",
    "    elif activation_func == \"relu\":\n",
    "        func = relu\n",
    "    elif activation_func == \"tanh\":\n",
    "        func = tanh\n",
    "    else:\n",
    "        raise Exception(' Function not valid')\n",
    "        \n",
    "    return func(Z), Z\n",
    "\n",
    "\n",
    "# Performs full forward propagation layer by layer.\n",
    "\n",
    "def full_forward(X, params_values, nn_architecture):\n",
    "    \n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        activ_function_curr = layer[\"activation_func\"]\n",
    "        W_curr = params_values[\"Weights\" + str(layer_idx)]\n",
    "        b_curr = params_values[\"bias\" + str(layer_idx)]\n",
    "        A_curr, Z_curr = one_layer_forward(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs backward propagation on one layer only\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation):\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation == \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    elif activation == \"same\":\n",
    "        backward_activation_func = same_backward\n",
    "    elif activation == \"tanh\":\n",
    "        backward_activation_func = tanh_backward\n",
    "    else:\n",
    "        raise Exception('{} activation is not supported'.format(activation))\n",
    "    \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "   \n",
    "    dA_prev = error(Y_hat,Y)\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        activ_function_curr = layer[\"activation_func\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        W_curr = params_values[\"Weights\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"bias\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    for layer_idx, layer in enumerate(nn_architecture):\n",
    "        params_values[\"Weights\" + str(layer_idx+1)] -= learning_rate * grads_values[\"dW\" + str(layer_idx+1)]        \n",
    "        params_values[\"bias\" + str(layer_idx+1)] -= learning_rate * grads_values[\"db\" + str(layer_idx+1)]\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
    "    params_values = initialize_nn_layers(nn_architecture, 0.1)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Y_hat, cashe = full_forward(X, params_values, nn_architecture)\n",
    "        loss = return_loss(Y_hat, Y)\n",
    "        cost_history.append(float(loss))\n",
    "\n",
    "        \n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "    return params_values, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a-\tThe direct problem of kinematic data initialisation\n",
    "a1 = 2\n",
    "a2 = 3\n",
    "\n",
    "teta1_vect= np.deg2rad(np.linspace(0,180,num=10000))\n",
    "np.random.shuffle(teta1_vect)\n",
    "teta2_vect= np.deg2rad(np.linspace(0,180,num=10000))\n",
    "np.random.shuffle(teta2_vect)\n",
    "\n",
    "x= (np.multiply(a1,np.cos(teta1_vect)) + np.multiply(a2,np.cos(np.add(teta1_vect,teta2_vect))))\n",
    "y= (np.multiply(a1,np.sin(teta1_vect)) + np.multiply(a2,np.sin(np.add(teta1_vect,teta2_vect))))\n",
    "\n",
    "def rescale(array):\n",
    "    # return array\n",
    "    return np.divide(np.subtract(array,array.min()),np.subtract(array.max(),array.min()))\n",
    "\n",
    "\n",
    "inputs = np.transpose(np.stack([rescale(teta1_vect),rescale(teta2_vect)], axis=1))\n",
    "outputs = np.transpose(np.stack([rescale(x),rescale(y)], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE is 0.03052617258294965 for [[2, 150, 'tanh'], [150, 75, 'tanh'], [75, 50, 'tanh'], [50, 25, 'tanh'], [25, 10, 'tanh'], [10, 2, 'same']]\n",
      "MAE is 0.09092225493261608 for [[2, 8, 'tanh'], [8, 16, 'tanh'], [16, 2, 'same']]\n",
      "MAE is 0.05448388974439323 for [[2, 8, 'same'], [8, 16, 'tanh'], [16, 2, 'same']]\n",
      "MAE is 0.2563429885212123 for [[2, 8, 'same'], [8, 16, 'same'], [16, 2, 'same']]\n",
      "MAE is 0.10305107044387089 for [[2, 8, 'tanh'], [8, 16, 'relu'], [16, 2, 'same']]\n",
      "MAE is 0.09092225493261608 for [[2, 8, 'tanh'], [8, 16, 'tanh'], [16, 2, 'same']]\n",
      "MAE is 0.0575664592546865 for [[2, 25, 'same'], [25, 50, 'tanh'], [50, 10, 'tanh'], [10, 2, 'same']]\n",
      "MAE is 0.0400052980753999 for [[2, 25, 'tanh'], [25, 50, 'tanh'], [50, 75, 'tanh'], [75, 50, 'tanh'], [50, 25, 'tanh'], [25, 2, 'same']]\n",
      "MAE is 0.13387466462087216 for [[2, 8, 'tanh'], [8, 2, 'same']]\n",
      "MAE is 0.036067920007514305 for [[2, 128, 'tanh'], [128, 64, 'tanh'], [64, 32, 'tanh'], [32, 16, 'tanh'], [16, 2, 'same']]\n"
     ]
    }
   ],
   "source": [
    "list_of_networks = []\n",
    "list_of_networks.append([[2,150,'tanh'],[150,75,'tanh'],[75,50,'tanh'],[50,25,'tanh'],[25,10,'tanh'],[10,2,'same']])\n",
    "list_of_networks.append([[2,8,'tanh'],[8,16,'tanh'],[16,2,'same']])\n",
    "list_of_networks.append([[2,8,'same'],[8,16,'tanh'],[16,2,'same']])\n",
    "list_of_networks.append([[2,8,'same'],[8,16,'same'],[16,2,'same']])\n",
    "list_of_networks.append([[2,8,'tanh'],[8,16,'relu'],[16,2,'same']])\n",
    "list_of_networks.append([[2,8,'tanh'],[8,16,'tanh'],[16,2,'same']])\n",
    "list_of_networks.append([[2,25,'same'],[25,50,'tanh'],[50,10,'tanh'],[10,2,'same']])\n",
    "list_of_networks.append([[2,25,'tanh'],[25,50,'tanh'],[50,75,'tanh'],[75,50,'tanh'],[50,25,'tanh'],[25,2,'same']])\n",
    "list_of_networks.append([[2,8,'tanh'],[8,2,'same']])\n",
    "list_of_networks.append([[2,128,'tanh'],[128,64,'tanh'],[64,32,'tanh'],[32,16,'tanh'],[16,2,'same']])\n",
    "\n",
    "\n",
    "for item in list_of_networks:\n",
    "   \n",
    "    prototype_network= dict_layers(item)\n",
    "    _, cost_history =train(inputs,outputs,prototype_network,10000, 0.1)\n",
    "    print(\"MAE is {} for {}\".format(np.array(cost_history).min(), item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b-\tThe inverse problem of kinematic data initialisation\n",
    "a1 = 2\n",
    "a2 = 3\n",
    "\n",
    "teta1_vect= np.deg2rad(np.linspace(0,180,num=40000))\n",
    "np.random.shuffle(teta1_vect)\n",
    "teta2_vect= np.deg2rad(np.linspace(0,180,num=40000))\n",
    "np.random.shuffle(teta2_vect)\n",
    "\n",
    "x= (np.multiply(a1,np.cos(teta1_vect)) + np.multiply(a2,np.cos(np.add(teta1_vect,teta2_vect))))\n",
    "y= (np.multiply(a1,np.sin(teta1_vect)) + np.multiply(a2,np.sin(np.add(teta1_vect,teta2_vect))))\n",
    "\n",
    "def rescale(array):\n",
    "    # return array\n",
    "    return np.divide(np.subtract(array,array.min()),np.subtract(array.max(),array.min()))\n",
    "\n",
    "\n",
    "outputs = np.transpose(np.stack([rescale(teta1_vect),rescale(teta2_vect)], axis=1))\n",
    "inputs = np.transpose(np.stack([rescale(x),rescale(y)], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE is 0.07868767486199724 for [[2, 150, 'tanh'], [150, 75, 'tanh'], [75, 50, 'tanh'], [50, 25, 'tanh'], [25, 10, 'tanh'], [10, 2, 'same']]\n",
      "MAE is 0.13620612901051468 for [[2, 8, 'tanh'], [8, 16, 'tanh'], [16, 2, 'same']]\n",
      "MAE is 0.0888599667592713 for [[2, 8, 'same'], [8, 16, 'tanh'], [16, 2, 'same']]\n",
      "MAE is 0.3337954569898536 for [[2, 8, 'same'], [8, 16, 'same'], [16, 2, 'same']]\n",
      "MAE is 0.1746188211483029 for [[2, 8, 'tanh'], [8, 16, 'relu'], [16, 2, 'same']]\n",
      "MAE is 0.13620612901051468 for [[2, 8, 'tanh'], [8, 16, 'tanh'], [16, 2, 'same']]\n",
      "MAE is 0.09319061872545518 for [[2, 25, 'same'], [25, 50, 'tanh'], [50, 10, 'tanh'], [10, 2, 'same']]\n",
      "MAE is 0.09066523030735546 for [[2, 25, 'tanh'], [25, 50, 'tanh'], [50, 75, 'tanh'], [75, 50, 'tanh'], [50, 25, 'tanh'], [25, 2, 'same']]\n",
      "MAE is 0.2016698071792191 for [[2, 8, 'tanh'], [8, 2, 'same']]\n",
      "MAE is 0.07415273912200762 for [[2, 128, 'tanh'], [128, 64, 'tanh'], [64, 32, 'tanh'], [32, 16, 'tanh'], [16, 2, 'same']]\n"
     ]
    }
   ],
   "source": [
    "list_of_networks = []\n",
    "list_of_networks.append([[2,150,'tanh'],[150,75,'tanh'],[75,50,'tanh'],[50,25,'tanh'],[25,10,'tanh'],[10,2,'same']])\n",
    "list_of_networks.append([[2,8,'tanh'],[8,16,'tanh'],[16,2,'same']])\n",
    "list_of_networks.append([[2,8,'same'],[8,16,'tanh'],[16,2,'same']])\n",
    "list_of_networks.append([[2,8,'same'],[8,16,'same'],[16,2,'same']])\n",
    "list_of_networks.append([[2,8,'tanh'],[8,16,'relu'],[16,2,'same']])\n",
    "list_of_networks.append([[2,8,'tanh'],[8,16,'tanh'],[16,2,'same']])\n",
    "list_of_networks.append([[2,25,'same'],[25,50,'tanh'],[50,10,'tanh'],[10,2,'same']])\n",
    "list_of_networks.append([[2,25,'tanh'],[25,50,'tanh'],[50,75,'tanh'],[75,50,'tanh'],[50,25,'tanh'],[25,2,'same']])\n",
    "list_of_networks.append([[2,8,'tanh'],[8,2,'same']])\n",
    "list_of_networks.append([[2,128,'tanh'],[128,64,'tanh'],[64,32,'tanh'],[32,16,'tanh'],[16,2,'same']])\n",
    "\n",
    "\n",
    "for item in list_of_networks:\n",
    "   \n",
    "    prototype_network= dict_layers(item)\n",
    "    _, cost_history =train(inputs,outputs,prototype_network,10000, 0.1)\n",
    "    print(\"MAE is {} for {}\".format(np.array(cost_history).min(), item))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d039d092d32c8f3d55b0183b49a3956ae4647ae2f3365b8cbc81c5c00d96d55b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
